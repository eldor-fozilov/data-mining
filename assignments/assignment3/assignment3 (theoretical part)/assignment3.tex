\documentclass{homework}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{IE30301-Datamining Assignment 3 (70 Points)}
\author{Eldor Fozilov}
\date{April 27, 2021}


\begin{document}
    \maketitle
    \exercise*
    Write a detailed description of the following concepts and their differences. (Explain at least 2 lines about each concepts and write differences in 1 sentence. If not, there are 3 points deduction per problem.   [20 pts, 5 pts for each.] \\
    \begin{enumerate}
        \item \textbf{Likelihood $\&$ Probability}  \\
        \textbf{Probability}, in simple words, can be defined as the chance that a given event will occur. It is calculated as the ratio of the number of outcomes in a sample space representing a given event of interest to the total number of possible outcomes in that sample space. \\
        The word \textbf{likelihood} is often interchangeably used used with the word \textbf{probability} in daily conversations. However, it has a different meaning in statistics and machine learning. In those areas, \textbf{likelihood} generally concerned with finding the best distribution of the data given a sample data set. It is represented as a function (called \textbf{likelihood function}), which calculates the joint probability distribution of given data as a function of model parameters. That function can be thought as the probability of obtaining the given data given the parameters. \\
        
        \item \textbf{Feature Selection $\&$ Feature Extraction} \\
        \textbf{Feature selection} is referred to the selection of a subset of features out of the original features with the aim of simplifying a model, reducing computational cost and the influence of noise, caused by redundant features in the model. So, the result of successful feature selection can be an improvement in the model accuracy and speed. \\
        \textbf{Feature extraction} serves similar purposes mentioned above, but it tries to achieve it in a different way compared to \textbf{feature selection}: instead of choosing which variables to include in a model, feature extraction methods create new variables from all given feature variables by combining them in various ways so that those new variables retain most of the useful information. For example PCA creates new variables by taking a \textbf{linear} combination of given feature variables. 
        \item \textbf{PC Score $\&$ PC loading} \\
        We know the fact that \textbf{PCA} produces a low-dimensional representation of a data set by finding a sequence of linear combinations of the variables that retain maximum variance, and are uncorrelated with each other. The particular values in those derived variable vectors are called \textbf{PC scores}. \\
        To find those \textbf{PC scores}, we need to take a dot product between each normalized sample record, which can be thought as being a vector, and a parameter vector. The values in that parameter vector are called \textbf{PC loadings}, which represent the contribution of each feature variable to the computation of \textbf{PC score}. Those \textbf{PC loadings} can also be interpreted as correlations between feature variables and the loading vector (principal axis) they belong to. So we can see that PC score and PC loading are closely related, but not the same thing.
        \item \textbf{Newton-Raphson Method $\&$ Gradient Descent} \\
        \textbf{Gradient descent} is an iterative optimization algorithm for finding a local minimum of a differentiable function using its first-order derivative. It is grounded in the idea that if a derivative of a multivariate function \textit{f} exists at a point \textbf{x}, then the direction of fastest decrease/descent in the function value will happen when if one goes from the point \textbf{x} in the direction of the negative gradient of \textit{f} at \textbf{x},  $-\nabla$\textit{f}(\textbf{x}). That negative gradient can be multiplied by a some value to increase or decrease the step size (change in value of \textbf{x}). \\
        
        \textbf{Newton-Raphson method} is an iterative method used for finding the roots of a twice differentiable function \textfbf{f}. The main difference between those two methods is that gradient descent algorithm is parametric in nature (it usually requires a learning rate $\mu$), while the other method does not need such a parameter, and thus we can apply it without worrying for changing any hyperparameter.
        
        
        
    \end{enumerate}
    \vspace{10mm}
    
    
    
    
    \exercise*    
    In multiple linear regression, we can estimate $\hat{\mathbf{\beta}}$ as follows by least square method.
    
    \begin{equation}
    \hat{\mathbf{\beta}}= \mathbf{(X^TX)^{-1}X^Ty} \in \mathbb{R}^{(p+1) \times 1}, \label{eq:3-2}
    \quad 
    \mathbf{X} \in \mathbb{R}^{N\times(p+1)},
    \quad
    \mathbf{y} \in \mathbb{R}^{N\times1}
    \end{equation}
    
    The regression model can be derived as follows:
    \begin{equation}
    \hat{\mathbf{y}} = \mathbf{X\hat{\beta}} = \mathbf{X(X^TX)^{-1}X^Ty} = \mathbf{Hy} 
    \end{equation}
    
    
    And in the above equation, $\mathbf{X(X^TX)^{-1}X^T}$ is specifically referred to as $\textbf{H}$, hat matrix.
    
    \begin{equation}
    \mathbf{H}= \mathbf{X(X^TX)^{-1}X^T} 
    \end{equation}
    
    \subsection{}
    Show that $\textbf{H}$ is symmetric ($\mathbf{H^T}= \mathbf{H}$) and idempotent ($\mathbf{H^2} = \mathbf{H}$). [3 pts]

    \textbf{\textcolor{red}{Answer}}: \\
    First we will show that the matrix $\mathbf{H}$ is symmetric.
    \[ \mathbf{H^T} = \left[\mathbf{X(X^TX)^{-1}X^T})\right]^{T} \]
    We will use the following property of the transpose applied when multiplying two matrices:
    \begin{equation}
        \mathbf{(AB)^{T}} = \mathbf{B^{T}A^{T}}
    \end{equation}
    
    Let's label the matrix $\mathbf{(X^TX)^{-1}X^T}$ as M. Then we write $\mathbf{H^{T}}$ as
    
    \begin{equation*}
        \mathbf{H^{T}} = \left[\mathbf{XM}\right]^T = \mathbf{M^TX^T} = 
        \mathbf{\left[(X^TX)^{-1}X^T\right]^TX^T} = \mathbf{\left[(X^T)^T\left((X^TX)^{-1}\right)^T\right]X^T}
    \end{equation*}
    We know that $\mathbf{(X^T)^T} = \mathbf{X}$. We can also show that $\mathbf{\left((X^TX)^{-1}\right)^T}$ = $\mathbf{(X^TX)^{-1}}$.
    
    Let $\mathbf{A} = \mathbf{(X^TX)^{-1}}$. Then $\mathbf{A^{-1}A} = \mathbf{I}$ and $\mathbf{AA^{-1}} = \mathbf{I}$. If we take the transpose of both sides in those equations and apply (2.4), then we get the following:
    \[  \mathbf{A^T(A^{-1})^T} = \mathbf{I^T} = \mathbf{I}, \quad \mathbf{(A^{-1})^TA^T} = \mathbf{I^T} = \mathbf{I} \]
    From the above equations, we can see that $\mathbf{(A^T)^{-1}} = \mathbf{(A^{-1})^T}$.
    
    So, $\mathbf{\left((X^TX)^{-1}\right)^T} = \mathbf{\left((X^TX)^T\right)^{-1}} = \mathbf{(X^TX)^{-1}}$
    
    Thus, $\mathbf{H^T}$ can be expresses as
    \begin{equation*}
        \mathbf{H^T} = \mathbf{\left[(X^T)^T\left((X^TX)^{-1}\right)^T\right]X^T} =
         \mathbf{X(X^TX)^{-1}X^T} = \mathbf{H}
    \end{equation*}
    
    Now, we will show that the matrix $\mathbf{H}$ is idempotent.
    \begin{equation*}
    \begin{gathered}
        \mathbf{H^2} = \mathbf{HH} = \mathbf{\left(X(X^TX)^{-1}X^T\right)\left(X(X^TX)^{-1}X^T\right)} = \\
        = \mathbf{X(X^TX)^{-1}(X^TX)(X^TX)^{-1}X^T} = \mathbf{XI(X^TX)^{-1}X^T} = \\
        = \mathbf{X(X^TX)^{-1}X^T} = \mathbf{H}
    \end{gathered}
    \end{equation*}
    \vspace{5mm}
    
    \subsection{}
        An estimator of a given parameter is said to be unbiased if its expected value is equal to the true value of the parameter. (E$[\hat{\theta}] = \theta$)
        \vspace{2mm}
        
        \textbf{Show that $MSE = \hat{\sigma}^2 = \dfrac{SSE}{N-p-1} = \dfrac{(\mathbf{y-\hat{y}})^T(\mathbf{y-\hat{y}})}{N-p-1} $ is unbiased estimator} through using following properties (2.5) - (2.9) \& results of above problem \textbf{2.1} [10 pts]
    
    \vspace{3mm}
    
    If c is a scalar, and \textbf{A} is a n $\times$ n square matrix,  ($c \in \mathbb{R}, \mathbf{A} \in \mathbb{R}^{n \times n}$)
    \begin{equation}
    c = \text{Tr}(c),
    \quad
    \text{Tr}(c\mathbf{A}) = c\text{Tr}(\mathbf{A}) 
    \end{equation}
    \vspace{3mm}
    If \textbf{A, B} are n $\times$ n square matrix that have same dimension, $(\mathbf{A, B} \in \mathbb{R}^{n \times n}$)
    \begin{equation}
    \text{Tr}(\mathbf{A+B}) = \text{Tr}(\mathbf{A}) + \text{Tr}(\mathbf{B}),
    \quad
    \text{Tr}(\mathbf{A-B}) = \text{Tr}(\mathbf{A}) - \text{Tr}(\mathbf{B})
    \end{equation}
    \vspace{3mm}
    If \textbf{A} is a n $\times$ n square matrix, $\textbf{A} \in \mathbb{R}^{n\times n}$
    \begin{equation}
    \text{E}[\text{Tr}(\textbf{A})] = \text{Tr}(\text{E}[\textbf{A}]) 
    \end{equation}
    \vspace{3mm}
    If \textbf{A} is a n $\times$ m matrix and \textbf{B} is a m $\times$ n matrix ($\textbf{A} \in \mathbb{R}^{n\times m}, \textbf{B} \in \mathbb{R}^{m\times n}$)
    \begin{equation}
    \text{Tr}(\textbf{AB})= \text{Tr}(\textbf{BA}) 
    \end{equation}
    \vspace{3mm}
    If \textbf{x} is random vector,
    \begin{equation}
    \text{Var}[\mathbf{x}] = \text{E}[\mathbf{xx^T}] - \text{E}[\mathbf{x}]\text{E}[\mathbf{x}]^T,
    \quad
    \text{E}[\mathbf{xx^T}] = \text{Var}[\mathbf{x}] + \text{E}[\mathbf{x}]\text{E}[\mathbf{x}]^T
    \end{equation}
    
    \textbf{\textcolor{red}{Answer}}: \\
    Finding the expectation of \textbf{SSE} will give us clue to the expectation of \textbf{MSE} since \textbf{MSE} is a just a SSE divided by $N-p-1$. So we will tackle it first.
    \begin{equation*}
    \begin{gathered}
        \text{E}[SSE] =  \text{E}[\mathbf{(y-\hat{y}})^T(\mathbf{y-\hat{y})}] = 
        \text{E}[\mathbf{(y-Hy})^T(\mathbf{y-Hy)}], \quad \text{where\ } \mathbf{H} = \mathbf{X(X^TX)^{-1}X^T} \\[4pt]
        \text{E}[\mathbf{(y-Hy})^T(\mathbf{y-Hy)}] = 
        \text{E}[\mathbf{\left((I-H)y\right)^T}\mathbf{\left((I-H)y\right)}] = \\[4pt]
        = \text{E}[\text{Tr}\big (\mathbf{\left((I-H)y\right)^T}\mathbf{\left((I-H)y\right)}\big)] \quad (\because \text{property (2.5)}) \\[4pt]
        \text{E}[\text{Tr}\big (\mathbf{\left((I-H)y\right)^T}\mathbf{\left((I-H)y\right)}\big)] = \text{E}[\text{Tr}\big (\mathbf{y^T(I-H)^T}\mathbf{(I-H)y)}\big)] = \\[4pt]
        = \text{E}[\text{Tr}\big (\mathbf{y^T(I-H)}\mathbf{(I-H)y}\big)] 
        \quad (\because \text{\textbf{H} and \textbf{I} are symmetric and thus (\textbf{I - H}) is also symmetric}) \\[4pt]
        \text{E}[\text{Tr}\big (\mathbf{y^T(I-H)}\mathbf{(I-H)y}\big)] = 
        \text{E}[\text{Tr}\big (\mathbf{y^T(I-H)^2y}\big)] = \\[4pt]
        = \text{E}[\text{Tr}\big (\mathbf{y^T(I-H)y}\big)] 
        \quad   (\because \text{\textbf{I} and \textbf{H} are idempotent and thus (\textbf{I - H}) is idempotent}) \\[4pt]
        \text{E}[\text{Tr}\big (\mathbf{y^T(I-H)y}\big)] = 
        \text{E}[\text{Tr}\big (\mathbf{(I-H)yy^T}\big)] 
        \quad (\because \text{property (2.8)}) \\[4pt]
        \text{E}[\text{Tr}\big (\mathbf{(I-H)yy^T}\big)] =
        \text{Tr}\big (\text{E}[\mathbf{(I-H)yy^T}]\big) 
        \quad (\because \text{property (2.7)}) \\[4pt]
        \text{Tr}\big (\text{E}[\mathbf{(I-H)yy^T}]\big) = 
        \text{Tr}\big (\mathbf{(I-H)}\text{E}[\mathbf{yy^T}]\big)
        \quad (\because \text{\textbf{X} is considered to be given, thus \textbf{H} is a constant}) \\[4pt]
        \text{Tr}\big (\mathbf{(I-H)}\text{E}[\mathbf{yy^T}]\big) = 
        \text{Tr}\big (\mathbf{(I-H)}\left(\text{Var}[\mathbf{y}] + \text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\right)\big) 
        \quad (\because \text{property (2.9)}) \\[4pt]
        \text{Tr}\big (\mathbf{(I-H)}\left(\text{Var}[\mathbf{y}] + \text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\right)\big) = 
        \text{Tr}\big (\mathbf{(I-H)}\left(\sigma^2\mathbf{I} + \text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\right)\big) = \\[4pt]
        = \text{Tr}\big(\mathbf{(I-H)}\sigma^2\mathbf{I} +  \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big) = 
        \text{Tr}\big(\mathbf{(I-H)}\sigma^2\mathbf{I}\big) + \text{Tr}\big(  \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big)
        \quad (\because \text{property (2.6)}) \\[4pt]
        \text{Tr}\big(\mathbf{(I-H)}\sigma^2\mathbf{I}\big) + \text{Tr}\big(  \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big) = 
        \text{Tr}\big(\sigma^2\mathbf{I} - \sigma^2\mathbf{H}\big) + \text{Tr}\big( \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big) = \\[4pt]
        = \text{Tr}\big(\sigma^2\mathbf{I}\big) - \text{Tr}\big(\sigma^2\mathbf{H}\big) + \text{Tr}\big( \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big)
        \quad (\because \text{property (2.6)}) \\[4pt]
        \text{Tr}\big(\sigma^2\mathbf{I}\big) - \text{Tr}\big(\sigma^2\mathbf{H}\big) + \text{Tr}\big( \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big) = \\[4pt] 
        = \sigma^2\text{Tr}\big(\mathbf{I}\big) - \sigma^2\text{Tr}\big(\mathbf{H}\big) + \text{Tr}\big( \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big)
        \quad (\because \text{property (2.5)}) \\[4pt]
    \end{gathered}
    \end{equation*}
    
    Since $\mathbf{I} \in \mathbb{R}^{N\times 1},\  \text{Tr}(\mathbf{I}) = N$. And $\text{Tr}(\mathbf{H}) = \text{Tr}(\mathbf{X(X^TX)^{-1}X^T}) = \text{Tr}(\mathbf{(X^TX)^{-1}(X^TX)})$ because of property (2.8). So, Tr(\textbf{H}) = Tr(\textbf{I}) = p + 1 ($\because I \in \mathbb{R}^{(p+1)\times(p+1)}$)
    
    \begin{equation*}
    \begin{gathered}
        \text{Tr}\big( \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big) =
        \text{Tr}\big( \mathbf{(I-X(X^TX)^{-1}X^T)}(\mathbf{X\beta})(\mathbf{X\beta})^T\big) = \\[4pt]
        = \text{Tr}\big( \big(\mathbf{X\beta-X(X^TX)^{-1}X^TX\beta}\big)(\mathbf{X\beta})^T\big) = \text{Tr}\big( \big(\mathbf{X\beta-XI\beta}\big)(\mathbf{X\beta})^T\big) = \\[4pt]
        = \text{Tr}\big(\big(\mathbf{X\beta-X\beta}\big)(\mathbf{X\beta})^T\big) = 
        \text{Tr}\big(\mathbf{0}(\mathbf{X\beta})^T\big), \quad \text{where \textbf{0} is a zero vector} \in \mathbb{R}^{N\times 1}\\[4pt]
        \text{Tr}\big(\mathbf{0}(\mathbf{X\beta})^T\big) = 
        \text{Tr}\big(\mathbf{0}\big), \quad \text{\textbf{0} now represents a zero matrix} \in \mathbb{R}^{N\times N} \\[4pt]
        \text{Tr}\big(\mathbf{0}\big) = 0 \quad \text{(a constant)}
    \end{gathered}
    \end{equation*}
    Therefore,
    \begin{equation*}
        \begin{gathered}
        \text{E}[SSE] = \sigma^2\text{Tr}\big(\mathbf{I}\big) - \sigma^2\text{Tr}\big(\mathbf{H}\big) + \text{Tr}\big( \mathbf{(I-H)}\text{E}[\mathbf{y}]\text{E}[\mathbf{y}]^T\big) = \\[4pt]
        = \sigma^2N - \sigma^2(p+1) + 0 = \sigma^2(N - p - 1)
        \end{gathered}
    \end{equation*}
    
    So, $\text{E}[MSE] = \text{E}\Big[\dfrac{SSE}{N-p-1}\Big] = \dfrac{1}{N-p-1}\text{E}[SSE] = \dfrac{1}{N-p-1}\sigma^2(N - p - 1) = \sigma^2$
    
    \vspace{5mm}

    \exercise*
    For matrix \textbf{A}, solve the following problems
    $$ \textbf{A} = \left( \begin{matrix} 6 & -3 \\ 5 & -2 \end{matrix} \right) $$

    \subsection{}
    Compute the eigenvalues $\lambda_{1}, \lambda_{2}$  $(\lambda_{1} < \lambda_{2})$ and its corresponding eigenvectors $v_{1}, v_{2}$ of matrix \textbf{A} [2 pts]
    
    \textbf{\textcolor{red}{Answer}}: \\
    Let's first compute eigenvalues.
    
    \begin{equation*}
    \begin{gathered}
        \text{det}\big(\text{\textbf{A}} - \lambda \textbf{I}\big) = 
        \text{det}\big(\left[
        \begin{matrix}
        6-\lambda & -3 \\
        5  & -2-\lambda
        \end{matrix}
        \right]\big) = (6-\lambda)(-2-\lambda) -(-3)\times5 = \\
        = \lambda^2 - 4\lambda + 3 = 0 \Rightarrow \boxed{\lambda_{1} = 3, \lambda_{2} = 1}
    \end{gathered}
    \end{equation*}
    Now we will find eigenvectors.
    \begin{equation*}
    \begin{gathered}
        \text{\textbf{A}}v_1 = \lambda_1 v_1 \quad \Rightarrow \quad \text{\textbf{A}}v_1 - \lambda_1 v_1 = \overline{0}, \quad \overline{0} = \left(\begin{matrix}
        0 \\
        0 \\
        \end{matrix}\right) \\
        \left(\text{\textbf{A}} - \lambda_1 \text{\textbf{I}}\right)v_1 = \overline{0} \\
        \left(\begin{matrix}
        6-\lambda_1 & -3 \\
        5  & -2-\lambda_1
        \end{matrix}\right)v_1 = 
        \left(\begin{matrix}
        6-3 & -3 \\
        5  & -2-3
        \end{matrix}\right)v_1 = 
        \left(\begin{matrix}
        3 & -3 \\
        5  & -5
        \end{matrix}\right)v_1 = \overline{0}
        \end{gathered}
    \end{equation*}
    We will perform Gauss-Jordan elimination (but we will not follow the exact algorithm since the matrix is too simple) to find the vector $v_1 = \left(\begin{matrix}
    v_{11} \\
    v_{12}
    \end{matrix}\right)$ and later the vector $v_2 = \left(\begin{matrix}
    v_{21} \\
    v_{22}
    \end{matrix}\right)$:
    \begin{equation*}
    \left(\begin{matrix}
        3 & -3 \\
        5  & -5
        \end{matrix}\right)v_1 = \overline{0} \Rightarrow
        \left(\begin{matrix}
        1 & -1 \\
        1  & -1
        \end{matrix}\right)v_1 = \overline{0} \Rightarrow
        \left(\begin{matrix}
        1 & -1 \\
        0  & 0
        \end{matrix}\right)v_1 = \overline{0} \Rightarrow
        v_{11} = v_{12}
    \end{equation*}
    We can freely choose a value for either $v_{11}$ or $v_{12}$. Let's choose a value of 1 for $v_{11}$, which will mean that $v_{12}$ will be equal to 1. So, $v_1 = \left(\begin{matrix}
    1 \\
    1 \\
    \end{matrix}\right)$
    
    Now, we will find $v_2$.
    
        \begin{equation*}
    \begin{gathered}
        \text{\textbf{A}}v_2 = \lambda_2 v_2 \quad \Rightarrow \quad \text{\textbf{A}}v_2 - \lambda_2 v_2 = \overline{0}, \quad \overline{0} = \left(\begin{matrix}
        0 \\
        0 \\
        \end{matrix}\right) \\
        \left(\text{\textbf{A}} - \lambda_2 \text{\textbf{I}}\right)v_2 = \overline{0} \\
        \left(\begin{matrix}
        6-\lambda_2 & -3 \\
        5  & -2-\lambda_2
        \end{matrix}\right)v_2 = 
        \left(\begin{matrix}
        6-1 & -3 \\
        5  & -2-1
        \end{matrix}\right)v_2 = 
        \left(\begin{matrix}
        5 & -3 \\
        5  & -3
        \end{matrix}\right)v_2 = \overline{0}
        \end{gathered}
    \end{equation*}
    

    \begin{equation*}
    \left(\begin{matrix}
        5 & -3 \\
        5  & -3
        \end{matrix}\right)v_2 = \overline{0} \Rightarrow
        \left(\begin{matrix}
        1 & -0.6 \\
        1  & -0.6
        \end{matrix}\right)v_2 = \overline{0} \Rightarrow
        \left(\begin{matrix}
        1 & -0.6 \\
        0  & 0
        \end{matrix}\right)v_2 = \overline{0} \Rightarrow
        v_{21} = 0.6v_{22}
    \end{equation*}
    We can freely choose a value for either $v_{21}$ or $v_{22}$. Let's choose a value of 1 for $v_{22}$, which will mean that $v_{21}$ will be equal to 0.6. So, $v_2 = \left(\begin{matrix}
    0.6 \\
    1 \\
    \end{matrix}\right)$
    
    
    \subsection{}
	Find matrix \textbf{P} to diagonalize \textbf{A}. Here \textbf{D} is a diagonal matrix of size 2 Ã— 2  [3 pts]
	\begin{center}
		$\textbf{P}^{-1}\textbf{A}\textbf{P} = \textbf{D}$
	\end{center}
	\textbf{\textcolor{red}{Answer}}: \\
	
	Let \textbf{V} = $\left[v_1 \ v_2\right] = \left[\begin{matrix}
	v_{11} & v_{21} \\
	v_{12} & v_{22} \\
	\end{matrix}\right]$, and $\mathbf{\Lambda} = \left[\begin{matrix}
	\lambda_1 & 0 \\
	0 & \lambda_2 \\
	\end{matrix}
	\right]$. \\
	
	Then the following equality holds:
	\begin{equation}
	    \text{\textbf{AV}} = \text{\textbf{V}}\mathbf{\Lambda} 
	\end{equation}
	
	Since the eigenvectors $v_1$ and $v_2$ that we found in problem 3.1 are independent, $\text{\textbf{V}}^{-1}$ exists, and thus we can multiply the both sides of the equation (3.1) by $\text{\textbf{V}}^{-1}$ from the left side:
	\begin{equation}
	    \begin{gathered}
	    \text{\textbf{V}}^{-1}\text{\textbf{AV}} = \text{\textbf{V}}^{-1}\text{\textbf{V}}\mathbf{\Lambda} \Rightarrow \\
	    \Rightarrow 
	    \text{\textbf{V}}^{-1}\text{\textbf{AV}} = \text{\textbf{I}}\mathbf{\Lambda} = \mathbf{\Lambda}
	    \end{gathered}
	\end{equation}
	   Thus, matrix \boxed{\text{\textbf{P}} = \text{\textbf{V}}}
	

	\subsection{}
	Compute the determinant of $\textbf{A}^{2521}$. The calculation should be trivial if you use the properties of determinant. [3 pts]
	
	\textbf{\textcolor{red}{Answer}}: \\
	
	We are aware of the following property of the determinant when we have two square matrices \textbf{A} and \textbf{B}, both $\in \mathbb{R}^{n\times n}$:
    \begin{equation}
        \text{det}\big(\text{\textbf{AB}}\big) = \text{det}\big(\text{\textbf{A}}\big)\text{det}\big(\text{\textbf{B}}\big)
    \end{equation}
    The \textbf{B} matrix can be \textbf{A} also, so we can write the property (3.3) as
    \begin{equation*}
    \begin{gathered}
        \text{det}\big(\text{\textbf{A}}^2\big) = \text{det}\big(\text{\textbf{A}}\big)\text{det}\big(\text{\textbf{A}}\big) \Rightarrow \\
        \text{det}\big(\text{\textbf{A}}^3\big) = \text{det}\big(\text{\textbf{A}}^2\big)\text{det}\big(\text{\textbf{A}}\big) = \text{det}\big(\text{\textbf{A}}\big)\text{det}\big(\text{\textbf{A}}\big)\text{det}\big(\text{\textbf{A}}\big)
    \end{gathered}
        \end{equation*}
    So, we can see the general trend as the power of \textbf{A} grows, and thus we can write the following:
	\begin{equation*}
	    \text{det}\left(\text{\textbf{A}}^{2521}\right) = \left[\text{det}\left(\text{\textbf{A}}\right)\right]^{2521}
	\end{equation*}
	Thus, if we can find the determinant of the matrix \textbf{A} alone, we can find the determinant of \textbf{A}$^{2521}$ easily.
    \begin{equation*}
    \begin{gathered}
        	\text{det}\left(\text{\textbf{A}}\right) = \text{det}\big(\left[
        \begin{matrix}
        6 & -3 \\
        5  & -2 \\
        \end{matrix}
        \right]\big) = 6\times(-2) - (-3)\times5 = 3 \Rightarrow \\
        \boxed{\text{det}\left(\text{\textbf{A}}^{2521}\right) = 3^{2521}}
    \end{gathered}
    \end{equation*}
	
	
	\vspace{10mm}
    
    \exercise*
    Given data \textbf{X}, solve following problems
    $$ \textbf{X} = \left( \begin{matrix} 1 & 3 & 9 \\ 2 & 5 & 7 \\ 4 & 4 & 6 \\ 9 & 8 & 2 \end{matrix} \right) $$
    i.e., data with four samples and three features (predictors).
    
    \subsection{}
    Find the mean value of each column. [1 pts]
    
    \textbf{\textcolor{red}{Answer}}: \\
    
    $\mu_1 = \dfrac{1+2+4+9}{4} = 4$ \\
    $\mu_2 = \dfrac{3+5+4+8}{4} = 5$ \\
    $\mu_3 = \dfrac{9+7+6+2}{4} = 6$ 
    
    \subsection{}
    Subtract each mean from each element of the corresponding column. \\
    Now, let us set the derived matrix as $\textbf{X}'$. [1 pts]
    
    \textbf{\textcolor{red}{Answer}}: \\
       $$ \textbf{X}' = \left( \begin{matrix} 1-4 & 3-5 & 9-6 \\ 2-4 & 5-5 & 7-6 \\ 4-4 & 4-5 & 6-6 \\ 9-4 & 8-5 & 2-6 \end{matrix} \right) =
        \left( \begin{matrix} -3 & -2 & 3 \\ -2 & 0 & 1 \\ 0 & -1 & 0 \\ 5 & 3 & -4 \end{matrix} \right) $$
    
    \subsection{}
    Calculate $\frac{1}{4-1}\textbf{X}'^T\textbf{X}'.$ [1 pts]
    
    \textbf{\textcolor{red}{Answer}}: \\
    \begin{equation*}
        \begin{gathered}
                \dfrac{1}{4-1}\textbf{X}'^T\textbf{X}' = \dfrac{1}{3}\left( \begin{matrix} -3 & -2 & 3 \\ -2 & 0 & 1 \\ 0 & -1 & 0 \\ 5 & 3 & -4 \end{matrix} \right)^{T}\left( \begin{matrix} -3 & -2 & 3 \\ -2 & 0 & 1 \\ 0 & -1 & 0 \\ 5 & 3 & -4 \end{matrix} \right) = \\
                = \dfrac{1}{3}\left( \begin{matrix} -3 & -2 & 0 & 5 \\ -2 & 0 & -1 & 3 \\ 3 & 1 & 0 & -4 \\ \end{matrix} \right)\left( \begin{matrix} -3 & -2 & 3 \\ -2 & 0 & 1 \\ 0 & -1 & 0 \\ 5 & 3 & -4 \end{matrix} \right) = \dfrac{1}{3}\left(\begin{matrix}
                38 & 21 & -31 \\
                21 & 14 & -18 \\
                -31 & -18 & 26 \\
                \end{matrix}\right) = \left(\begin{matrix}
                \frac{38}{3} & 7 & \frac{-31}{3} \\
                7 & \frac{14}{3} & -6 \\
                \frac{-31}{3} & -6 & \frac{26}{3} \\
                \end{matrix}\right)
        \end{gathered}
    \end{equation*}
    
    \subsection{}
    For the calculated matrix in \textbf{4.3}, find eigenvalues $\lambda_{1},\lambda_{2},\lambda_{3}$ in descending order  $(\lambda_{1} > \lambda_{2} > \lambda_{3})$ up to four decimal places. [2 pts] \\
    (\href{<https://www.symbolab.com/solver/matrix-eigenvalues-calculator/eigenvalues>}{https://www.symbolab.com/solver/matrix-eigenvalues-calculator/eigenvalues})
    
    \textbf{\textcolor{red}{Answer}}:
    
    $\lambda_1 = 25.3084$ \\
    $\lambda_2 = 0.6044$ \\
    $\lambda_3 = 0.0872$
    
    \subsection{}
    Calculate $\dfrac{\lambda_{1}}{(\lambda_{1} +\lambda_{2} + \lambda_{3})}$. (up to four decimal places) [2 pts]
    
    \textbf{\textcolor{red}{Answer}}:
    
    $\dfrac{\lambda_{1}}{(\lambda_{1} +\lambda_{2} + \lambda_{3})} = \dfrac{25.3084}{25.3084 + 0.6044 + 0.0872} = 0.9734$
    
    \subsection{}
    Find eigenvectors $\textbf{a}_{1}, \textbf{a}_{2}, \textbf{a}_{3}$ corresponding to $\lambda_{1}, \lambda_{2}, \lambda_{3}$. [2 pts] \\
    (\href{<https://www.symbolab.com/solver/matrix-eigenvalues-calculator/eigenvectors>}{https://www.symbolab.com/solver/matrix-eigenvalues-calculator/eigenvectors})    
    
    \textbf{\textcolor{red}{Answer}}: 
    
    $\textbf{a}_1 = \left(\begin{matrix}
    -1.2045 \\
    -0.6992 \\
    1 \\
    \end{matrix}\right)$, \quad 
    $\textbf{a}_2 = \left(\begin{matrix}
    138.5863 \\
    -237.3327 \\
    1 \\
    \end{matrix}\right)$, \quad
    $\textbf{a}_3 = \left(\begin{matrix}
    0.6182 \\
    0.3652 \\
    1 \\
    \end{matrix}\right)$
    
    \vspace{10mm}
     
    \exercise*

    Consider three random variables $X,Y,Z$. The three variables have the covariance matrix in the form of: 
    $$ \Sigma = \left( \begin{matrix} a & ka & 0 \\ ka & a & ka \\ 0 & ka & a \end{matrix} \right) $$, where $ 0 < k < \frac{1}{\sqrt{2}}$\\


    \subsection{}
    Calculate the eigenvalues $\lambda_1,\lambda_2,\lambda_3$. (Show process of calculation neatly) [4 pts]
    
    \textbf{\textcolor{red}{Answer}}: 
    \begin{center}
    \includegraphics[width=\textwidth, angle = -90]{5.1.jpg}
    \end{center}
    \subsection{}
    Find PC(Principal Component) $\textbf{p}_1,\textbf{p}_2,\textbf{p}_3$ of each random variable $X,Y,Z$. [3 pts]
    
    \textbf{\textcolor{red}{Answer}}:
    
    \includegraphics[width=\textwidth, angle = -90, scale = 1.2]{5.2(1).jpg} \\
    \includegraphics[width=\textwidth, angle = -90, scale = 1.2]{5.2(2).jpg}
    
    \subsection{}
    Calculate how much total variance is explained by each principal component. [3 pts]
    
    \textbf{\textcolor{red}{Answer}}:
    
    \includegraphics[width=\textwidth]{5.3.jpg}
    
    \vspace{10mm}
        
        \exercise*
    The following table is the outcome of the logistic regression model for an iris flower being species Versicolor versus species Virginica. ($Y=1$ for Versicolor, $Y=0$ for Virginica) [9 pts]
    \begin{table}[!h]
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Variables & Intercept  & Length of Sepals  & Width of Sepals  & Length of petals  & Width of Petals  \\ \hline
    Estimated  & 25.21  & -3  & -0.7  & 2.4  & -10.3&
     Coefficient& &&&& \\ \hline
    \end{tabular}
    \end{center}
    \end{table}

    
    \subsection{}
    Interpret the effect of length of sepals on the relative risks of an iris flower being species Versicolor versus species Virginica. Fill up the \textbf{A,B,C} in the below interpretation of the outcome and select the appropriate word for \textbf{D}. (Tips from TA : Think about the meaning of odds(X)!) [4 pts] \\

    \textbf{\textcolor{red}{Answer}}: 
    
    \textbf{Interpretation}: Species \textbf{Virginica} is $\dfrac{1}{e^{-3}} = 20$ times more probable than Species \textbf{Versicolor} when the length of sepals D \textbf{increases} by 1 unit.
    
    \subsection{}
    What is the predicted class for the following new data $\mathbf{x}_1$ and $\mathbf{x}_2$? You should provide the probability of $P(Y_{1}=1|\mathbf{x}_{1}),P(Y_{1}=0|\mathbf{x}_{1}),P(Y_{2}=1|\mathbf{x}_{2}),P(Y_{2}=0|\mathbf{x}_{2}).$ [6 pts]
    \begin{table}[!h]
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{x}_{1} & 6.4  & 3.1  & 4.3  & 1.3   \\ \hline
    \textbf{x}_{2}  & 6.9  & 3.0  & 3.9  & 1.4  \\ \hline
    \end{tabular}
    \end{center}
    \end{table}
    
    \textbf{\textcolor{red}{Answer}}: 
    
    \large{$    P(Y_{1}=1|\mathbf{x}_{1}) = \dfrac{1}{1+ e^{-(25.21 + (-3)\times6.4 + (-0.7)\times3.1 + 2.4\times4.3 + (-10.3)\times1.3)}} = \dfrac{1}{1+e^{-0.77}} = 0.6835 
    $}
    
    \large{$P(Y_{1}=0|\mathbf{x}_{1}) = 1 - P(Y_{1}=1|\mathbf{x}_{1}) = 1 - 0.6835 = 0.3165$}
    
    So, the predicted class for $\mathbf{x}_1$ will be \textbf{Versicolor}.
    
       \large{$    P(Y_{2}=1|\mathbf{x}_{2}) = \dfrac{1}{1+ e^{-(25.21 + (-3)\times6.9 + (-0.7)\times3.0 + 2.4\times3.9 + (-10.3)\times1.4)}} = \dfrac{1}{1+e^{2.65}} = 0.066 
    $}
    
    \large{$P(Y_{2}=0|\mathbf{x}_{2}) = 1 - P(Y_{2}=1|\mathbf{x}_{2}) = 1 - 0.066 = 0.934$}
    
    So, the predicted class for $\mathbf{x}_2$ will be \textbf{Virginica}.
    
    
    
    

    \vspace{10mm}   
    
     
  
\end{document}
